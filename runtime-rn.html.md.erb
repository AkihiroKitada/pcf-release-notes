---
title: Pivotal Elastic Runtime v1.12 Release Notes
owner: Release Engineering
---

Pivotal Cloud Foundry is certified by the Cloud Foundry Foundation for 2017.

Read more about the [certified provider
program](https://www.cloudfoundry.org/provider-faq/) and the [requirements of
providers](https://www.cloudfoundry.org/provider-requirements/).

<hr>

## <a id='releases'></a> Releases

### <a id='1.12.0'></a> 1.12.0

#### <a id='component-versions'></a> Component Versions

  <table border="1" class="nice">
  <thead>
  <tr>
    <th>Component</th>
    <th>Version</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>Stemcell</td><td>3445.7</td>
  </tr><tr>
    <td>binary-offline-buildpack</td><td>1.0.14</td>
  </tr><tr>
    <td>capi</td><td>1.40.0\*</td>
  </tr><tr>
    <td>cf-autoscaling</td><td>95</td>
  </tr><tr>
    <td>cf-backup-and-restore</td><td>0.0.9</td>
  </tr><tr>
    <td>cf-mysql</td><td>36.5.0</td>
  </tr><tr>
    <td>cf-networking</td><td>1.4.0</td>
  </tr><tr>
    <td>cf-smoke-tests</td><td>38</td>
  </tr><tr>
    <td>cflinuxfs2</td><td>1.146.0</td>
  </tr><tr>
    <td>consul</td><td>173</td>
  </tr><tr>
    <td>diego</td><td>1.25.1</td>
  </tr><tr>
    <td>dotnet-core-offline-buildpack</td><td>1.0.23</td>
  </tr><tr>
    <td>garden-runc</td><td>1.9.3</td>
  </tr><tr>
    <td>go-offline-buildpack</td><td>1.8.6</td>
  </tr><tr>
    <td>grootfs</td><td>0.25.0</td>
  </tr><tr>
    <td>haproxy</td><td>8.4.0</td>
  </tr><tr>
    <td>java-offline-buildpack</td><td>4.5</td>
  </tr><tr>
    <td>loggregator</td><td>96</td>
  </tr><tr>
    <td>mysql-backup</td><td>1.35.0</td>
  </tr><tr>
    <td>mysql-monitoring</td><td>8.8.0</td>
  </tr><tr>
    <td>nats</td><td>22</td>
  </tr><tr>
    <td>nfs-volume</td><td>1.0.9</td>
  </tr><tr>
    <td>nodejs-offline-buildpack</td><td>1.6.4</td>
  </tr><tr>
    <td>notifications</td><td>37</td>
  </tr><tr>
    <td>notifications-ui</td><td>29</td>
  </tr><tr>
    <td>php-offline-buildpack</td><td>4.3.39</td>
  </tr><tr>
    <td>pivotal-account</td><td>1.6.1</td>
  </tr><tr>
    <td>push-apps-manager-release</td><td>662.0.11</td>
  </tr><tr>
    <td>push-usage-service-release</td><td>663.0.2</td>
  </tr><tr>
    <td>python-offline-buildpack</td><td>1.5.22</td>
  </tr><tr>
    <td>routing</td><td>0.162.0</td>
  </tr><tr>
    <td>ruby-offline-buildpack</td><td>1.6.46</td>
  </tr><tr>
    <td>scalablesyslog</td><td>10</td>
  </tr><tr>
    <td>service-backup</td><td>18.1.2</td>
  </tr><tr>
    <td>staticfile-offline-buildpack</td><td>1.4.12</td>
  </tr><tr>
    <td>statsd-injector</td><td>1.0.29</td>
  </tr><tr>
    <td>syslog-migration</td><td>8</td>
  </tr><tr>
    <td>uaa</td><td>45</td>
  </tr>
  </tbody>
  <tfoot>
  <tr>
    <td colspan="2"><em>* Components marked with an asterisk have been patched to resolve security vulnerabilities or fix component behavior.</em></td>
  </tr>
  </tfoot>
  </table>

## <a id='upgrade'></a> How to Upgrade

The procedure for upgrading to Pivotal Cloud Foundry (PCF) Elastic Runtime v1.12 is
documented in the [Upgrading Pivotal Cloud
Foundry](../customizing/upgrading-pcf.html) topic.

When upgrading to v1.12, be aware of the following upgrade considerations:

* You must upgrade first to a version of Elastic Runtime v1.11.x to
  successfully upgrade to v1.12.
* Some partner service tiles may be incompatible with PCF v1.12. Pivotal is
  working with partners to ensure their tiles are being updated to work with the
  latest versions of PCF.<br/><br/> For information about which partner service
  releases are currently compatible with PCF v1.12, review the appropriate
  partners services release documentation at
  [https://docs.pivotal.io](https://docs.pivotal.io), or contact the partner
  organization that produces the tile.

## <a id='1.12.0'></a> New Features in Elastic Runtime v1.12.0

This section describes new features of the release.

###<a id='multi-buildpack'></a> Multiple Buildpack App Support

Developers can now push apps that take advantage of multiple buildpacks simultaneously. You can upgrade apps to use multiple buildpacks with the [`cf v3-push` command](../buildpacks/use-multiple-buildpacks.html). This makes binaries, libraries, and language modules provided by all specified buildpacks available to the app. The final buildpack specified controls how the app starts.

Multiple buildpack support adds flexibility to the Cloud Foundry app development model. You can now use the official Cloud Foundry buildpacks together to support polyglot (multiple language) apps. Additionally, you can specify [custom buildpacks](../buildpacks/developing-buildpacks.html) before official Cloud Foundry buildpacks to supply dependencies that previously had to be provided with apps.

###<a id='credhub'></a> Migration of Internal Credentials to CredHub

Internal credentials, the `secret` and `simple_credentials` that Elastic Runtime uses for inter-component communication, are generated and stored in CredHub instead of Ops Manager. This is part of an ongoing effort to migrate all credentials to CredHub, which will reduce the amount of places credentials are stored, aid in credential rotation, and increase security.  

If you want to access the following credentials, you must use the CredHub CLI or the Ops Manager API instead of the **Credentials** tab of the Elastic Runtime tile. For instructions on how to to retrieve Elastic Runtime credentials, see the [Retrieving Credentials from Your Deployment](../customizing/credentials.html).

* `.mysql.autoscale_credentials`
* `.mysql.ccdb_credentials`
* `.mysql.diag_agent_credentials`
* `.mysql.diegodb_credentials`
* `.mysql.locketdb_credentials`
* `.mysql.monitordb_credentials`
* `.mysql.mysql_backup_server_credentials`
* `.mysql.mysql_bootstrap_credentials`
* `.mysql.networkpolicyserverdb_credentials`
* `.mysql.nfsvolume_credentials`
* `.mysql.notifications_credentials`
* `.mysql.pivotal_account_credentials`
* `.mysql.routingdb_credentials`
* `.mysql.silkdb_credentials`
* `.mysql.uaadb_credentials`
* `.nfsbrokerpush.nfs_broker_push_credentials`
* `.cloud_controller.bulk_api_credentials`
* `.cloud_controller.internal_api_user_credentials`
* `.cloud_controller.staging_upload_credentials`
* `.mysql.app_usage_credentials`
* `.mysql.cluster_health_user`
* `.mysql.galera_sidecar_user`
* `.mysql.mysql_admin_credentials`
* `.mysql_proxy.dashboard_credentials`
* `.nfs_server.blobstore_credentials`
* `.router.status_credentials`

###<a id='groot'></a> Introducing GrootFS

GrootFS is the new container image management plugin for Garden-runC.
It helps with the filesystem isolation of Garden-runC containers,
image caching,
and disk quota enforcement.
GrootFS replaces the previous built-in functionality,
which used an obsolete layer filesystem (AUFS)
that lacks support from the Linux Kernel community.
Additionally, GrootFS uses [OCI](https://www.opencontainers.org/) standards
for [container images](http://github.com/opencontainers/image-spec).

For more information about GrootFS in PCF, see the following topics:

* [Understanding Container Security](../concepts/container-security.html#mechanics)
* [Component: Garden](../concepts/architecture/garden.html#garden-rootfs)

###<a id='app-id-creds'></a> Application Instance Identity Credentials

The instance identity system in Diego provides each app container with a PEM-encoded [X.509](https://www.ietf.org/rfc/rfc5280.txt) certificate and [PKCS #1 RSA](https://tools.ietf.org/html/rfc3447) private key. The values of the environment variables `CF_INSTANCE_CERT` and `CF_INSTANCE_KEY` contain the absolute paths to the certificate and private key files. The validity period is 3 years for the Instance Identity root and 2 years for the intermediate CA certificates.

See the [App Instance Container Identity Credentials](../security/networking/tls-info.html#container-creds) section for more information.

###<a id='networking-poe'></a> Simplified TLS Configuration

The point of entry options on the Elastic Runtime **Networking** pane have been restructured to be
more understandable and flexible.  Operators no longer need to configure
the Router or HAProxy separately as both components are configured using
the same options. This includes the following changes:

* Gorouter and HAProxy always listen for TLS requests. You provide an SSL certificate for both Gorouter and HAProxy using a single field. 
* HAProxy forwards all requests to Gorouter over TLS by default. You can optionally disable this feature. 
* You can configure the minimum version of TLS for Gorouter and HAProxy with a single field. 
* You can provide a list of CAs to HAProxy for it to validate the Gorouter certificate. 
* You can optionally disable the HTTP listener for both Gorouter and HAProxy with a single checkbox. 
* You can specify TLS cipher suites for HAProxy and Gorouter independently.


See the Elastic Runtime [installation instructions](../customizing/gcp-er-config.html#networking) for your IaaS for more information.

###<a id='app-header'></a> Mutual TLS Headers on Inbound Application Traffic

Gorouter can now forward the `X-Forwarded-Client-Cert` header to app
instances when provided. Alternatively, operators can configure Gorouter to
forward the header only when the mutual TLS connection from the client can be
validated. Additionally, operators may now configure Gorouter to overwrite the
XFCC header with the client certificate received in mTLS handshakes.

This configuration is available in the **Networking** pane under **Configure the CF Router support for the X-Forwarded-Client-Cert header**. See the Elastic Runtime [installation instructions](../customizing/gcp-er-config.html#networking) for your IaaS for more information.

###<a id='cc-diego'></a> Secure Communication Between Cloud Controller and Diego

In previous versions of PCF, the Diego Brain VM ran the Cloud Controller Bridge
component, which translated Cloud Controller requests into Diego API commands.
The Cloud Controller Bridge conveyed communications between the Cloud
Controller and Diego over plaintext HTTP. In PCF v1.12, the Cloud Controller
and Diego communicate directly via secure TLS protocol. This change streamlines
and secures internal communications, and removes the Cloud Controller Bridge.

Securing this communication path will require a second deployment after
completing your upgrade to PCF v1.12. Follow the steps in our [upgrade guide](../customizing/upgrading-pcf.html#cc) to
secure your PCF installation.

###<a id='diego-metron'></a> Secure Communication Between Diego and Loggregator

Diego Cells now use the Metron API v2. This gRPC-based API supports mutual TLS authentication and secures the communication path between the Diego `rep` and Loggregator.

###<a id='scaling-loggregator'></a> Scaling Loggregator

As part of this release, the Loggregator team has provided guidelines for scaling the Loggregator system. For more information, see [Scaling Loggregator](../loggregator/log-ops-guide.html#scaling) and [Scaling Nozzles](../loggregator/log-ops-guide.html#scaling-nozzles) and [Operator Guidebook](https://github.com/cloudfoundry/loggregator/blob/develop/README.md#operator-guidebook).

###<a id='haproxy-release'></a> HAProxy Release

This release removes the old HAProxy job, which was the last remaining component from `cf-release`. It now uses the newly incubated
[haproxy-boshrelease](https://github.com/cloudfoundry-incubator/haproxy-boshrelease). This replacement allows Elastic Runtime to expose new HAProxy features, such as [request filtering](#request-filtering). 

###<a id='request-filtering'></a> HAProxy Request Filtering

If your PCF deployment uses HAProxy and you want it to receive traffic only from specific sources, you can use the **Protected Domains** and **Trusted CIDRs** fields in the **Networking Pane** of the Elastic Runtime tile. A key use case for this feature is when a deployment must only allow requests to the system domain from a private network or VPN. See the Elastic Runtime [installation instructions](../customizing/gcp-er-config.html#networking) for your IaaS for more information. 

###<a id='c2c'></a> Container-to-Container Networking Updates

Container-to-container networking is now always enabled. The commands are integrated with the cf CLI and now include the option to specify a port range when adding and removing policies. See [Create Policies for Container-to-Container Networking](../devguide/deploy-apps/cf-networking.html#create-policies). 

###<a id='app-traffic'></a> Support for Logging All App Traffic

Operators can enable logging of all accepted and denied packets due to ASGs or container-to-container networking policies. This provides more visibility into app traffic, including denied traffic.

Operators configure this global logging in the **Networking** pane of the Elastic Runtime tile under the **Log traffic for all accepted/denied application packets** field. See the [App Traffic Logging](../opsguide/logging-config-opsman.html#app-traffic-log) section for more information. 

###<a id='orpha-blobs'></a> Orphaned Blob Cleanup

The Cloud Controller now scans the blobstore on a regular interval to identify and remove orphaned blobs. For more information, see the [Blobstore](../concepts/architecture/cloud-controller.html#blob-store) section of the _Cloud Controller_ topic. 

###<a id='sharding'></a> Router Sharding Mode

This release includes support for router sharding between the Elastic Runtime and Isolation Segment tiles. Operators can choose to have the Elastic Runtime tile
routers only acknowledge requests from apps deployed within the its
Cells, or reject requests for any isolation segment. 

You can configure this feature using the following fields:

* Elastic Runtime tile: **Routers reject requests for Isolation Segments** checkbox
* Isolation Segment tile: **Router Sharding Mode** selector

See the Elastic Runtime [installation instructions](../customizing/gcp-er-config.html#networking) for your IaaS for more information.

###<a id='max-conn'></a> Gorouter Max Connection Configuration

Operators can limit the number of app instance connections to the backend using the **Max Connections Per Backend** field in the **Networking** pane of the Elastic Runtime tile. This field can help prevent malicious apps from consuming all available Gorouter resources. See the Elastic Runtime [installation instructions](../customizing/gcp-er-config.html#networking) for your IaaS for more information. 

###<a id='gcr'></a> Authenticating with Google Container Registry (GCR) to Push Docker Images

For PCF v1.12 and later, Pivotal recommends authenticating with GCR using the procedure documented in the following section: [Push a Docker Image from Google Container Registry (GCR)](../devguide/deploy-apps/push-docker.html#gcr). The alternative authentication mechanism provided by GCR passes a short lived (12 hours) access token to PCF. This enables PCF to pull images from GCR during the initial `cf push`, but subsequent `restage`, `push`, or rescheduling operations fail once the access token expires.

###<a id='ldap-volume'></a> NFSV3 Volume Services with LDAP

Operators can now configure LDAP for NFSv3 volume services. Using LDAP secures the NFSV3 volume service by preventing a developer from binding to an NFS share using an
arbitrary UID and potentially gaining access to sensitive data stored by another user or app. If you enable LDAP support, 
 developers must provide credentials for any user they wish
to bind as. See [Enabling NFS Volume Services](../opsguide/enable-vol-services.html). 

###<a id='mysql-metrics'></a> Metrics for MySQL

The internal MySQL job included in Elastic Runtime now emits metrics. See the [Elastic Runtime MySQL KPIs](../monitoring/kpi.html#kpi4MySQL).

###<a id='cloudform'></a> CloudFormation Template Improvements 

This release includes an improved CloudFormation template file available with the Elastic Runtime tile on [Pivotal Network](https://network.pivotal.io). The new template creates three availability zones, a load balancer for TCP routing, and the Ops Manager VM. For updated installation instructions, see [Installing PCF on AWS Using CloudFormation](../customizing/cloudform-install.html).

###<a id='max-in-flight'></a> Diego Cell Max-in-Flight Default

This release lowers the default max-in-flight percentage on 
Diego Cells to 4%. Previously, this value was set to 10%, which can cause the following issues in larger environments:

* Many simultaneous VM creates/deletes and BOSH blob updates placing
significant stress on the underlying infrastructure, especially on vSphere
which has a greater probability of being under-provisioned.
* Cells that are draining are no longer available for allocation, resulting
in a 10% decrease in total memory and disk capacity during deployment. This
can cause deployments to no longer have sufficient total capacity to run all work, or to have insufficient headroom to place larger workloads
successfully.

Operators can still use the Ops Manager API to configure this setting to fit their needs. For more information about this property, see [Managing Diego Cell Limits During Upgrade](../adminguide/diego-cell-upgrade.html).

###<a id='etcd-gone'></a> Removal of etcd

This release removes the `etcd` server VMs from the PCF deployment.  Operators
must ensure they are deploying service tiles that are known to be compatible
with PCF Elastic Runtime 1.12.

###<a id='postgres-gone'></a> Removal of Postgres

This release removes the legacy Postgres databases for the Cloud Controller and
UAA.  If your deployment was originally installed before PCF 1.6 and still uses
Postgres, you must contact your dedicated Support Engineer or Platform
Architect for assistance migrating your Cloud Controller and UAA databases to
MySQL. They will have access to the PostgreSQL-to-MySQL Migrator tool and
instructions on Pivotal Network.

If you do not migrate to MySQL before upgrading to Elastic Runtime 1.12, the
upgrade will fail.

### <a id='in-context-service'></a>Apps Manager: In-context Service Creation

Developers can create services without leaving the app or space view for an accelerated workflow.

### <a id='service-config-discovery'></a>Apps Manager: Service Configuration Parameter Discovery

When creating a new service in Apps Manager, developers can discover additional parameter options as fields, or a JSON editor that enables them to define the parameters.

##<a id='knownissues'></a> Known Issues

### <a id='credhub-restart'></a> Manual CredHub Restart Required During an Elastic Runtime Redeploy

In Elastic Runtime v1.12.0, the BOSH Backup and Restore (BBR) script does not restart the CredHub process. When following the [Restoring Pivotal Cloud Foundry from Backup with BBR](../customizing/backup-restore/restore-pcf-bbr.html#redeploy-ert) procedure, the Elastic Runtime redeploy fails after clicking **Apply Changes** since CredHub requires a restart.

To work around this issue, manually restart the CredHub process on the BOSH Director by running `monit restart credhub`, then click **Apply Changes**.

For more information, see the corresponding [Knowledge Base](https://discuss.pivotal.io/hc/en-us/articles/115012475527) article.

###<a id='autoscaler'></a> Lack of Autoscaler Scaling

You cannot scale the Autoscaler job to greater than one instance.

## <a id='adv-features'></a> About Advanced Features

The Advanced Features section of the Elastic Runtime tile includes new
functionality that may have certain constraints.

Although these features are fully supported, Pivotal recommends caution when
using them in production.
